# RAG-Alloy â€” .env.example (MVP)
# Notes:
# - Booleans must be lowercase: true/false
# - Keep this file in the repo root as `.env` (copy from this template)
# - Python runtime is enforced at startup: 3.11.x only

# --- Runtime ---
PYTHON_VERSION=3.11
APP_NAME=rag-alloy
APP_ENV=dev
APP_PORT=8080
APP_LOG_LEVEL=INFO

# Auth: token|none (write endpoints require token when token mode is on)
APP_AUTH_MODE=token
APP_TOKEN=change_me

# CORS (comma-separated origins or *)
CORS_ALLOW_ORIGINS=*

# Upload limits (MB)
MAX_UPLOAD_MB=50

# --- Storage / Collections ---
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=rag_alloy_default
# QDRANT_API_KEY=            # not required for local docker

# Optional data/cache directories
DATA_DIR=./data
UPLOAD_DIR=./data/uploads
CACHE_DIR=./.cache

# --- Retrieval & Fusion ---
RETRIEVAL_DEFAULT_MODE=hybrid    # semantic|lexical|hybrid
RETRIEVAL_TOP_K=8
FUSION_METHOD=rrf                # reciprocal-rank-fusion

# Chunking defaults
CHUNK_SIZE=800
CHUNK_OVERLAP=120

# --- Graph (optional) ---
GRAPH_ENABLED=false
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=change_me

# --- Generation (optional; disabled by default) ---
GEN_PROVIDER=none                # none|transformers|ollama
# Local HF transformers model name or path (download separately if needed)
TRANSFORMERS_MODEL=Qwen2.5-0.5B-Instruct
# Ollama settings (if using ollama)
OLLAMA_HOST=http://127.0.0.1:11434
OLLAMA_MODEL=llama3:instruct

# --- Embeddings ---
# CPU-friendly default; can be overridden with a local path
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# --- OCR (future hook; off in MVP) ---
OCR_ENABLED=false
OCR_ENGINE=tesseract

# --- Observability ---
METRICS_ENABLED=true
REQUEST_TIMEOUT_S=60
UVICORN_WORKERS=1
UVICORN_RELOAD=false

# --- Privacy / Safety ---
ANALYTICS_OPT_IN=false
LOG_REDACT_CONTENT=true
ALLOW_DELETE_COLLECTIONS=true
